# Images
images:
  airflow:
    repository: localhost:5001/custom-local-airflow
    tag: latest
    pullPolicy: Always

# Airflow executor
# One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor
executor: "KubernetesExecutor"

# If this is true and using LocalExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the scheduler's
# service account will have access to communicate with the api-server and launch pods.
# If this is true and using CeleryExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the workers
# will be able to launch pods.
allowPodLaunching: true

# Environment variables for all airflow containers
env: 
- name: "MINIO_ENDPOINT"
  value: "http://minio.minio.svc.cluster.local:9000"
- name: "MINIO_ACCESS_KEY"
  value: "service-user-challenge-data-pipeline"
- name: "KAFKA_BOOTSTRAP_SERVERS"
  value: "kafka.kafka.svc.cluster.local:9092"
- name: "AIRFLOW_CONN_SPARK_DEFAULT"
  value: "spark://k8s://https://127.0.0.1:6443"

# Secrets for all airflow containers
secret: []
# - envName: ""
#   secretName: ""
#   secretKey: ""  
    
# Flower settings
flower:
  enabled: false

# StatsD settings
statsd:
  enabled: false

# Git sync
dags:
  gitSync:
    enabled: true

    repo: https://github.com/cesarbruschetta/challenge-data-pipeline
    branch: main
    rev: HEAD
    depth: 1
    subPath: "dags"
  
logs:
  persistence:
    enabled: true
    storageClassName: "nfs"
    size: 1Gi
